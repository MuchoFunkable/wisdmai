{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b45baf",
   "metadata": {},
   "source": [
    "# Twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bba2794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/dennis/opt/anaconda3/lib/python3.9/site-packages (4.8.0)\r\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /Users/dennis/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (3.2.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /Users/dennis/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (2.28.1)\r\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /Users/dennis/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (1.3.1)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/dennis/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dennis/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2022.6.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dennis/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/dennis/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.11)\r\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'API' object has no attribute 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m search_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#amazon -filter:retweets\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# get tweets from the API\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m tweets \u001b[38;5;241m=\u001b[39m tw\u001b[38;5;241m.\u001b[39mCursor(\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m,\n\u001b[1;32m     20\u001b[0m               q\u001b[38;5;241m=\u001b[39msearch_query,\n\u001b[1;32m     21\u001b[0m               lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m               since\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2022-06-16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mitems(\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# store the API responses in a list\u001b[39;00m\n\u001b[1;32m     25\u001b[0m tweets_copy \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'API' object has no attribute 'search'"
     ]
    }
   ],
   "source": [
    "#'https://datascienceparichay.com/article/get-data-from-twitter-api-in-python-step-by-step-guide/'\n",
    "\n",
    "!pip3 install tweepy\n",
    "\n",
    "# import tweepy\n",
    "import tweepy as tw\n",
    "\n",
    "# your Twitter API key and API secret\n",
    "my_api_key = \"\"\n",
    "my_api_secret = \"\"\n",
    "\n",
    "# authenticate\n",
    "auth = tw.OAuthHandler(my_api_key, my_api_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "search_query = \"#amazon -filter:retweets\"\n",
    "\n",
    "# get tweets from the API\n",
    "tweets = tw.Cursor(api.search,\n",
    "              q=search_query,\n",
    "              lang=\"en\",\n",
    "              since=\"2022-06-16\").items(50)\n",
    "\n",
    "# store the API responses in a list\n",
    "tweets_copy = []\n",
    "for tweet in tweets:\n",
    "    tweets_copy.append(tweet)\n",
    "    \n",
    "print(\"Total Tweets fetched:\", len(tweets_copy))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# intialize the dataframe\n",
    "tweets_df = pd.DataFrame()\n",
    "\n",
    "# populate the dataframe\n",
    "for tweet in tweets_copy:\n",
    "    hashtags = []\n",
    "    try:\n",
    "        for hashtag in tweet.entities[\"hashtags\"]:\n",
    "            hashtags.append(hashtag[\"text\"])\n",
    "        text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "    except:\n",
    "        pass\n",
    "    tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name, \n",
    "                                               'user_location': tweet.user.location,\\\n",
    "                                               'user_description': tweet.user.description,\n",
    "                                               'user_verified': tweet.user.verified,\n",
    "                                               'date': tweet.created_at,\n",
    "                                               'text': text, \n",
    "                                               'hashtags': [hashtags if hashtags else None],\n",
    "                                               'source': tweet.source}))\n",
    "    tweets_df = tweets_df.reset_index(drop=True)\n",
    "\n",
    "# show the dataframe\n",
    "tweets_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'https://medium.com/dataseries/how-to-scrape-millions-of-tweets-using-snscrape-195ee3594721'\n",
    "\n",
    "!pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list2 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('COVID Vaccine since:2021-01-01 until:2021-05-31').get_items()):\n",
    "    if i>5000:\n",
    "        break\n",
    "    tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfcdf0",
   "metadata": {},
   "source": [
    "# Reddit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca779728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c\n",
    "#https://datascienceparichay.com/article/get-data-from-twitter-api-in-python-step-by-step-guide/\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# we use this function to convert responses to dataframes\n",
    "def df_from_response(res):\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()['data']['children']:\n",
    "        df = df.append({\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            'title': post['data']['title'],\n",
    "            'selftext': post['data']['selftext'],\n",
    "            'upvote_ratio': post['data']['upvote_ratio'],\n",
    "            'ups': post['data']['ups'],\n",
    "            'downs': post['data']['downs'],\n",
    "            'score': post['data']['score'],\n",
    "            'link_flair_css_class': post['data']['link_flair_css_class'],\n",
    "            'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            'id': post['data']['id'],\n",
    "            'kind': post['kind'],\n",
    "            'comments': post['data']['num_comments'],\n",
    "            'author':post['data']['author'],\n",
    "            'crossposts':post['data']['num_crossposts'],\n",
    "            'created': post['data']['created_utc'],\n",
    "            'seconds_passed': time.time() - post['data']['created_utc']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "with open('pw.txt', 'r') as f:\n",
    "    pw = f.read()\n",
    "\n",
    "# authenticate API\n",
    "client_auth = requests.auth.HTTPBasicAuth('ZLJQUDUpM5tgeaJJ_MfbjA', 'JwKa8UDlUr_KxPDkrmQF1AJuFrzysg')\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': 'docmouseMcFlavor',\n",
    "    'password': pw\n",
    "}\n",
    "headers = {'User-Agent': 'myBot/0.0.1'}\n",
    "\n",
    "# send authentication request for OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                    auth=client_auth, data=data, headers=headers)\n",
    "# extract token from response and format correctly\n",
    "token = f\"bearer {res.json()['access_token']}\"\n",
    "# update API headers with authorization (bearer token)\n",
    "headers = {**headers, **{'Authorization': token}}\n",
    "\n",
    "# initialize dataframe and parameters for pulling data in loop\n",
    "data = pd.DataFrame()\n",
    "#increase the limit to get more data\n",
    "params = {'limit': 5}\n",
    "\n",
    "# loop through 10 times (returning 1K posts)\n",
    "for i in range(3):\n",
    "    # make request\n",
    "    res = requests.get(\"https://oauth.reddit.com/r/wallstreetbets/new\",\n",
    "                       headers=headers,\n",
    "                       params=params)\n",
    "\n",
    "    # get dataframe from response\n",
    "    new_df = df_from_response(res)\n",
    "    # take the final row (oldest entry)\n",
    "    row = new_df.iloc[len(new_df)-1]\n",
    "    # create fullname\n",
    "    fullname = row['kind'] + '_' + row['id']\n",
    "    # add/update fullname in params\n",
    "    params['after'] = fullname\n",
    "    \n",
    "    # append new_df to data\n",
    "    data = data.append(new_df, ignore_index=True)\n",
    "\n",
    "users = data['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb65344",
   "metadata": {},
   "outputs": [],
   "source": [
    "karma = []\n",
    "for user in users:    \n",
    "    res = requests.get(\"https://oauth.reddit.com/user/\"+user+\"/about\",\n",
    "                           headers=headers,\n",
    "                           params=params)\n",
    "    karma.append(res.json()['data']['total_karma'])\n",
    "data['author_total_karma'] = karma\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "335d8f5b",
   "metadata": {},
   "source": [
    "#Upvotes/downvotes\n",
    "#Comments\n",
    "User history (longevity/activity)\n",
    "#crossposts\n",
    "#karma\n",
    "#Post-date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "influence_scores = []\n",
    "for i in range(len(data)):\n",
    "    influence_scores.append(((1+math.exp(-1*data.iloc[i]['author_total_karma']))**(-1))*\n",
    "                            ((1+0.5*math.exp(-1*data.iloc[i]['score']))**(-1))*\n",
    "                             ((1+math.exp(-1*data.iloc[i]['comments']))**(-1))*\n",
    "                             ((1+math.exp(-1*data.iloc[i]['crossposts']))**(-1))*\n",
    "                            (math.exp((-1)*data.iloc[i]['seconds_passed']/86400)))\n",
    "data['influence_score'] = influence_scores\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da52af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "\n",
    "model1 = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(string, model, type = None):\n",
    "    #string - text to run through model \n",
    "    #model - model url (reference above) \n",
    "    #output types: score, label \n",
    "    done = False\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer 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\"}\n",
    "    while not done:\n",
    "        try: \n",
    "            #access model + obtain ouput\n",
    "            payload = query = {\"inputs\": string}\n",
    "            print(payload)\n",
    "            response = requests.post(model, headers = headers, json = query) \n",
    "            print(response.json())\n",
    "            output = response.json()[0]\n",
    "            #print(output)\n",
    "\n",
    "            best = max(output, key = lambda x: x['score'])\n",
    "            label = best['label'].lower()\n",
    "            score = np.round(best['score'], decimals = 3)\n",
    "            done = True \n",
    "        except Exception as KeyError: \n",
    "            pass\n",
    "            if KeyError:\n",
    "                sleep(20)  \n",
    "    \n",
    "    #desired output\n",
    "    if type == \"score\": \n",
    "        return score\n",
    "    if type == \"label\": \n",
    "        return label\n",
    "\n",
    "    return label, score\n",
    "\n",
    "vec_sentiment = np.vectorize(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e3e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(data['selftext'][0], model1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cef5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = []\n",
    "sentiment = []\n",
    "for i in range(len(data)):\n",
    "    sentiment.append(get_sentiment(data['selftext'][i], model1)[0])\n",
    "    sentiment_scores.append(get_sentiment(data['selftext'][i], model1)[1])\n",
    "data['sentiment_score'] = sentiment_scores\n",
    "data['sentiment'] = sentiment\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b030fcb",
   "metadata": {},
   "source": [
    "# NYSE/Yahoo Finance  \n",
    "\n",
    "- read up on api \n",
    "- write up code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709fff42",
   "metadata": {},
   "source": [
    "# WSJ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea671a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
